{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIME: Competition Math Reasoning Strategies\n",
    "\n",
    "This notebook compares different strategies on AIME (American Invitational Mathematics Examination) problems.\n",
    "\n",
    "AIME is significantly harder than GSM8K:\n",
    "- Competition-level problems\n",
    "- Requires deeper reasoning\n",
    "- Answers are integers 0-999\n",
    "\n",
    "**Strategies compared:**\n",
    "- Greedy decoding\n",
    "- Self-consistency (majority voting)\n",
    "- Tree search methods (Best-first, MCTS)\n",
    "- Extended thinking (more tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.models import VLLMModel\n",
    "from src.datasets import AIMEDataset\n",
    "from src.samplers import (\n",
    "    GreedySampler, StandardSampler, DiverseSampler,\n",
    "    BestFirstTreeSearch, MCTSTreeSearch\n",
    ")\n",
    "from src.evaluators import AccuracyEvaluator, MajorityVotingEvaluator\n",
    "from src.runners import run_evaluation\n",
    "from src.utils import save_results\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "MAX_PROBLEMS = 30  # AIME has ~450 problems total, but they're hard\n",
    "OUTPUT_DIR = \"../results/aime_comparison\"\n",
    "\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "print(\"Loading model...\")\n",
    "model = VLLMModel(\n",
    "    model_name=MODEL_NAME,\n",
    "    gpu_memory_utilization=0.9,\n",
    "    dtype=\"auto\",\n",
    ")\n",
    "print(f\"Model loaded: {model.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = AIMEDataset(source=\"hf\")\n",
    "print(f\"Dataset: {dataset.name}, {len(dataset)} problems\")\n",
    "\n",
    "problems = dataset.get_problems(limit=MAX_PROBLEMS)\n",
    "print(f\"\\nUsing {len(problems)} problems for evaluation\")\n",
    "\n",
    "# Preview\n",
    "print(f\"\\nExample problem:\")\n",
    "print(f\"Prompt: {problems[0].prompt[:300]}...\")\n",
    "print(f\"Gold answer: {problems[0].gold_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Experiments\n",
    "\n",
    "For AIME, we expect tree search methods to be more beneficial due to problem difficulty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = {\n",
    "    # Baseline\n",
    "    \"greedy\": {\n",
    "        \"sampler\": GreedySampler(max_tokens=2048),\n",
    "        \"evaluator\": AccuracyEvaluator(dataset),\n",
    "        \"n_samples\": 1,\n",
    "        \"description\": \"Greedy decoding\",\n",
    "    },\n",
    "    \n",
    "    # Extended thinking (more tokens)\n",
    "    \"greedy_long\": {\n",
    "        \"sampler\": GreedySampler(max_tokens=4096),\n",
    "        \"evaluator\": AccuracyEvaluator(dataset),\n",
    "        \"n_samples\": 1,\n",
    "        \"description\": \"Greedy with extended tokens (4096)\",\n",
    "    },\n",
    "    \n",
    "    # Self-consistency\n",
    "    \"self_consistency_8\": {\n",
    "        \"sampler\": StandardSampler(temperature=0.8, top_p=0.95, max_tokens=2048),\n",
    "        \"evaluator\": MajorityVotingEvaluator(dataset),\n",
    "        \"n_samples\": 8,\n",
    "        \"description\": \"Self-consistency (8 samples)\",\n",
    "    },\n",
    "    \n",
    "    # Higher temperature self-consistency (more exploration)\n",
    "    \"self_consistency_high_temp\": {\n",
    "        \"sampler\": StandardSampler(temperature=1.0, top_p=0.95, max_tokens=2048),\n",
    "        \"evaluator\": MajorityVotingEvaluator(dataset),\n",
    "        \"n_samples\": 8,\n",
    "        \"description\": \"Self-consistency (8 samples, temp=1.0)\",\n",
    "    },\n",
    "    \n",
    "    # Diverse sampling\n",
    "    \"diverse_16\": {\n",
    "        \"sampler\": DiverseSampler(\n",
    "            temperatures=[0.5, 0.7, 0.9, 1.0, 1.2],\n",
    "            top_p=0.95,\n",
    "            max_tokens=2048\n",
    "        ),\n",
    "        \"evaluator\": MajorityVotingEvaluator(dataset),\n",
    "        \"n_samples\": 16,\n",
    "        \"description\": \"Diverse temperatures (16 samples)\",\n",
    "    },\n",
    "    \n",
    "    # Best-first tree search (smaller budget)\n",
    "    \"best_first_small\": {\n",
    "        \"sampler\": BestFirstTreeSearch(\n",
    "            max_expansions=20,\n",
    "            branch_factor=3,\n",
    "            max_tokens=512,\n",
    "            tokens_per_step=64,\n",
    "            temperature=0.8,\n",
    "        ),\n",
    "        \"evaluator\": AccuracyEvaluator(dataset),\n",
    "        \"n_samples\": 1,\n",
    "        \"description\": \"Best-first tree (20 expansions)\",\n",
    "    },\n",
    "    \n",
    "    # Best-first tree search (larger budget)\n",
    "    \"best_first_large\": {\n",
    "        \"sampler\": BestFirstTreeSearch(\n",
    "            max_expansions=50,\n",
    "            branch_factor=4,\n",
    "            max_tokens=768,\n",
    "            tokens_per_step=48,\n",
    "            temperature=0.8,\n",
    "        ),\n",
    "        \"evaluator\": AccuracyEvaluator(dataset),\n",
    "        \"n_samples\": 1,\n",
    "        \"description\": \"Best-first tree (50 expansions)\",\n",
    "    },\n",
    "    \n",
    "    # MCTS (smaller budget)\n",
    "    \"mcts_small\": {\n",
    "        \"sampler\": MCTSTreeSearch(\n",
    "            max_iterations=30,\n",
    "            branch_factor=3,\n",
    "            max_tokens=512,\n",
    "            tokens_per_step=64,\n",
    "            rollout_tokens=128,\n",
    "            temperature=0.8,\n",
    "            exploration_constant=1.5,\n",
    "        ),\n",
    "        \"evaluator\": AccuracyEvaluator(dataset),\n",
    "        \"n_samples\": 1,\n",
    "        \"description\": \"MCTS (30 iterations)\",\n",
    "    },\n",
    "    \n",
    "    # MCTS (larger budget, more exploration)\n",
    "    \"mcts_large\": {\n",
    "        \"sampler\": MCTSTreeSearch(\n",
    "            max_iterations=80,\n",
    "            branch_factor=4,\n",
    "            max_tokens=768,\n",
    "            tokens_per_step=48,\n",
    "            rollout_tokens=128,\n",
    "            temperature=0.8,\n",
    "            exploration_constant=2.0,  # More exploration\n",
    "        ),\n",
    "        \"evaluator\": AccuracyEvaluator(dataset),\n",
    "        \"n_samples\": 1,\n",
    "        \"description\": \"MCTS (80 iterations, high exploration)\",\n",
    "    },\n",
    "}\n",
    "\n",
    "print(f\"Defined {len(experiments)} experiments:\")\n",
    "for name, exp in experiments.items():\n",
    "    print(f\"  - {name}: {exp['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "\n",
    "for exp_name, exp_config in experiments.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running: {exp_name}\")\n",
    "    print(f\"Description: {exp_config['description']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Tree search methods need batch_size=1\n",
    "    is_tree = \"tree\" in exp_name or \"mcts\" in exp_name\n",
    "    batch_size = 1 if is_tree else (2 if exp_config[\"n_samples\"] > 1 else 4)\n",
    "    \n",
    "    results, metrics, responses, scores = run_evaluation(\n",
    "        model=model,\n",
    "        sampler=exp_config[\"sampler\"],\n",
    "        dataset=dataset,\n",
    "        evaluator=exp_config[\"evaluator\"],\n",
    "        batch_size=batch_size,\n",
    "        n_samples=exp_config[\"n_samples\"],\n",
    "        max_problems=MAX_PROBLEMS,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    elapsed = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    run_dir = save_results(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        run_name=exp_name,\n",
    "        results=results,\n",
    "        metrics=metrics,\n",
    "        config={\n",
    "            \"experiment\": exp_name,\n",
    "            \"description\": exp_config[\"description\"],\n",
    "            \"n_samples\": exp_config[\"n_samples\"],\n",
    "            \"model\": MODEL_NAME,\n",
    "        },\n",
    "        responses=responses,\n",
    "        scores=scores,\n",
    "    )\n",
    "    \n",
    "    all_results[exp_name] = {\n",
    "        \"metrics\": metrics,\n",
    "        \"results\": results,\n",
    "        \"elapsed\": elapsed,\n",
    "        \"n_samples\": exp_config[\"n_samples\"],\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nCompleted in {elapsed:.1f}s\")\n",
    "    print(f\"Accuracy: {metrics.accuracy:.4f} ({metrics.correct}/{metrics.total})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "for exp_name, data in all_results.items():\n",
    "    metrics = data[\"metrics\"]\n",
    "    comparison_data.append({\n",
    "        \"Experiment\": exp_name,\n",
    "        \"Description\": experiments[exp_name][\"description\"],\n",
    "        \"Accuracy\": metrics.accuracy,\n",
    "        \"Correct\": metrics.correct,\n",
    "        \"Total\": metrics.total,\n",
    "        \"N Samples\": data[\"n_samples\"],\n",
    "        \"Time (s)\": data[\"elapsed\"],\n",
    "        \"Time/Problem (s)\": data[\"elapsed\"] / metrics.total,\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "df = df.sort_values(\"Accuracy\", ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Accuracy comparison\n",
    "ax1 = axes[0]\n",
    "colors = ['#2ecc71' if 'mcts' in name or 'tree' in name else \n",
    "          '#3498db' if 'consistency' in name or 'diverse' in name else \n",
    "          '#e74c3c' for name in df[\"Experiment\"]]\n",
    "bars = ax1.barh(df[\"Experiment\"], df[\"Accuracy\"], color=colors)\n",
    "ax1.set_xlabel(\"Accuracy\")\n",
    "ax1.set_title(\"AIME Accuracy by Strategy\")\n",
    "ax1.set_xlim(0, max(df[\"Accuracy\"]) * 1.2 if df[\"Accuracy\"].max() > 0 else 0.3)\n",
    "\n",
    "for bar, acc in zip(bars, df[\"Accuracy\"]):\n",
    "    ax1.text(acc + 0.005, bar.get_y() + bar.get_height()/2, \n",
    "             f'{acc:.3f}', va='center', fontsize=10)\n",
    "\n",
    "# Time comparison\n",
    "ax2 = axes[1]\n",
    "bars = ax2.barh(df[\"Experiment\"], df[\"Time/Problem (s)\"], color=colors)\n",
    "ax2.set_xlabel(\"Time per Problem (seconds)\")\n",
    "ax2.set_title(\"Compute Time by Strategy\")\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#2ecc71', label='Tree Search'),\n",
    "    Patch(facecolor='#3498db', label='Self-Consistency'),\n",
    "    Patch(facecolor='#e74c3c', label='Greedy'),\n",
    "]\n",
    "ax1.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/aime_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy vs Time scatter\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "# Group by method type\n",
    "for exp_name, data in all_results.items():\n",
    "    metrics = data[\"metrics\"]\n",
    "    \n",
    "    if 'mcts' in exp_name:\n",
    "        color, marker = '#2ecc71', 's'  # Green square\n",
    "    elif 'tree' in exp_name:\n",
    "        color, marker = '#27ae60', '^'  # Darker green triangle\n",
    "    elif 'consistency' in exp_name or 'diverse' in exp_name:\n",
    "        color, marker = '#3498db', 'o'  # Blue circle\n",
    "    else:\n",
    "        color, marker = '#e74c3c', 'D'  # Red diamond\n",
    "    \n",
    "    ax.scatter(\n",
    "        data[\"elapsed\"] / metrics.total,\n",
    "        metrics.accuracy,\n",
    "        s=150,\n",
    "        c=color,\n",
    "        marker=marker,\n",
    "        alpha=0.8,\n",
    "        edgecolors='black',\n",
    "        linewidth=1,\n",
    "    )\n",
    "    ax.annotate(\n",
    "        exp_name.replace('_', '\\n'),\n",
    "        (data[\"elapsed\"] / metrics.total, metrics.accuracy),\n",
    "        xytext=(8, 0),\n",
    "        textcoords='offset points',\n",
    "        fontsize=8,\n",
    "        va='center',\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"Time per Problem (seconds)\", fontsize=12)\n",
    "ax.set_ylabel(\"Accuracy\", fontsize=12)\n",
    "ax.set_title(\"AIME: Accuracy vs Compute Tradeoff\", fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/aime_pareto.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Problem Difficulty Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze which problems each method solves\n",
    "method_correct = {}\n",
    "for exp_name, data in all_results.items():\n",
    "    method_correct[exp_name] = {r.problem_id: r.correct for r in data[\"results\"]}\n",
    "\n",
    "# Count how many methods solve each problem\n",
    "problem_solve_counts = {}\n",
    "for pid in method_correct[\"greedy\"].keys():\n",
    "    count = sum(1 for method in method_correct.values() if method.get(pid, False))\n",
    "    problem_solve_counts[pid] = count\n",
    "\n",
    "# Distribution\n",
    "counts = list(problem_solve_counts.values())\n",
    "n_methods = len(all_results)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist(counts, bins=range(n_methods + 2), align='left', rwidth=0.8, color='steelblue')\n",
    "ax.set_xlabel(\"Number of Methods that Solved Problem\")\n",
    "ax.set_ylabel(\"Number of Problems\")\n",
    "ax.set_title(\"Problem Difficulty Distribution\")\n",
    "ax.set_xticks(range(n_methods + 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/problem_difficulty.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Problems solved by ALL methods: {sum(1 for c in counts if c == n_methods)}\")\n",
    "print(f\"Problems solved by NO method: {sum(1 for c in counts if c == 0)}\")\n",
    "print(f\"Problems solved by SOME methods: {sum(1 for c in counts if 0 < c < n_methods)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find problems uniquely solved by tree search\n",
    "tree_methods = [name for name in all_results.keys() if 'mcts' in name or 'tree' in name]\n",
    "other_methods = [name for name in all_results.keys() if name not in tree_methods]\n",
    "\n",
    "unique_tree_solves = []\n",
    "for pid in method_correct[\"greedy\"].keys():\n",
    "    tree_solved = any(method_correct[m].get(pid, False) for m in tree_methods)\n",
    "    other_solved = any(method_correct[m].get(pid, False) for m in other_methods)\n",
    "    if tree_solved and not other_solved:\n",
    "        unique_tree_solves.append(pid)\n",
    "\n",
    "print(f\"\\nProblems uniquely solved by tree search methods: {len(unique_tree_solves)}\")\n",
    "\n",
    "# Show examples\n",
    "if unique_tree_solves:\n",
    "    print(\"\\nExample problems uniquely solved by tree search:\")\n",
    "    for pid in unique_tree_solves[:2]:\n",
    "        for p in problems:\n",
    "            if p.id == pid:\n",
    "                print(f\"\\n--- {pid} ---\")\n",
    "                print(f\"Question: {p.prompt[:400]}...\")\n",
    "                print(f\"Gold answer: {p.gold_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: AIME Strategy Comparison\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_df = df[[\"Experiment\", \"Accuracy\", \"Correct\", \"Total\", \"Time/Problem (s)\"]].copy()\n",
    "summary_df[\"Accuracy\"] = summary_df[\"Accuracy\"].apply(lambda x: f\"{x:.4f}\")\n",
    "summary_df[\"Time/Problem (s)\"] = summary_df[\"Time/Problem (s)\"].apply(lambda x: f\"{x:.2f}\")\n",
    "\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Analysis\n",
    "best_exp = df.iloc[0]\n",
    "greedy_acc = all_results[\"greedy\"][\"metrics\"].accuracy\n",
    "\n",
    "print(f\"\\n\\nKey Findings:\")\n",
    "print(f\"  1. Best accuracy: {best_exp['Experiment']} ({best_exp['Accuracy']})\")\n",
    "print(f\"  2. Greedy baseline: {greedy_acc:.4f}\")\n",
    "print(f\"  3. Best improvement over greedy: +{float(best_exp['Accuracy']) - greedy_acc:.4f}\")\n",
    "\n",
    "# Tree search analysis\n",
    "tree_accs = [all_results[m][\"metrics\"].accuracy for m in tree_methods if m in all_results]\n",
    "other_accs = [all_results[m][\"metrics\"].accuracy for m in other_methods if m in all_results]\n",
    "\n",
    "if tree_accs and other_accs:\n",
    "    print(f\"\\n  Tree search methods avg accuracy: {np.mean(tree_accs):.4f}\")\n",
    "    print(f\"  Other methods avg accuracy: {np.mean(other_accs):.4f}\")\n",
    "\n",
    "df.to_csv(f\"{OUTPUT_DIR}/summary.csv\", index=False)\n",
    "print(f\"\\nResults saved to {OUTPUT_DIR}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
