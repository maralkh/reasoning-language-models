{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IFEval: Instruction Following Evaluation\n",
    "\n",
    "This notebook analyzes model performance on instruction following tasks from IFEval.\n",
    "\n",
    "**IFEval is unique because:**\n",
    "- Tests verifiable instruction constraints (word count, format, keywords, etc.)\n",
    "- Each problem has multiple constraints to satisfy\n",
    "- We can analyze which types of constraints are hardest\n",
    "\n",
    "**Analysis includes:**\n",
    "- Overall accuracy\n",
    "- Accuracy by constraint type\n",
    "- Effect of number of constraints on success\n",
    "- Sampling strategy comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "from src.models import VLLMModel\n",
    "from src.datasets import IFEvalDataset\n",
    "from src.samplers import GreedySampler, StandardSampler, DiverseSampler\n",
    "from src.evaluators import AccuracyEvaluator, MajorityVotingEvaluator\n",
    "from src.runners import run_evaluation\n",
    "from src.utils import save_results\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "MAX_PROBLEMS = 100\n",
    "OUTPUT_DIR = \"../results/ifeval_comparison\"\n",
    "\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading model...\")\n",
    "model = VLLMModel(\n",
    "    model_name=MODEL_NAME,\n",
    "    gpu_memory_utilization=0.9,\n",
    ")\n",
    "print(f\"Model loaded: {model.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = IFEvalDataset()\n",
    "print(f\"Dataset: {dataset.name}, {len(dataset)} problems\")\n",
    "\n",
    "problems = dataset.get_problems(limit=MAX_PROBLEMS)\n",
    "print(f\"Using {len(problems)} problems\")\n",
    "\n",
    "# Preview\n",
    "print(f\"\\nExample problem:\")\n",
    "print(f\"Prompt: {problems[0].prompt[:300]}...\")\n",
    "print(f\"\\nConstraints: {problems[0].gold_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyze Constraint Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count constraint types\n",
    "constraint_counts = defaultdict(int)\n",
    "constraints_per_problem = []\n",
    "\n",
    "for problem in problems:\n",
    "    constraints = problem.gold_answer\n",
    "    constraints_per_problem.append(len(constraints))\n",
    "    for c in constraints:\n",
    "        ctype = c.type.split(':')[0] if ':' in c.type else c.type\n",
    "        constraint_counts[ctype] += 1\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1 = axes[0]\n",
    "sorted_counts = sorted(constraint_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "types, counts = zip(*sorted_counts[:15])\n",
    "ax1.barh(range(len(types)), counts, color='steelblue')\n",
    "ax1.set_yticks(range(len(types)))\n",
    "ax1.set_yticklabels(types)\n",
    "ax1.set_xlabel('Count')\n",
    "ax1.set_title('Constraint Types in Dataset')\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "ax2 = axes[1]\n",
    "ax2.hist(constraints_per_problem, bins=range(1, max(constraints_per_problem) + 2), \n",
    "         align='left', rwidth=0.8, color='coral')\n",
    "ax2.set_xlabel('Number of Constraints')\n",
    "ax2.set_ylabel('Number of Problems')\n",
    "ax2.set_title('Constraints per Problem')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/constraint_distribution.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average constraints per problem: {np.mean(constraints_per_problem):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = {\n",
    "    \"greedy\": {\n",
    "        \"sampler\": GreedySampler(max_tokens=1024),\n",
    "        \"evaluator\": AccuracyEvaluator(dataset),\n",
    "        \"n_samples\": 1,\n",
    "        \"description\": \"Greedy decoding\",\n",
    "    },\n",
    "    \"temp_0.3\": {\n",
    "        \"sampler\": StandardSampler(temperature=0.3, top_p=0.95, max_tokens=1024),\n",
    "        \"evaluator\": AccuracyEvaluator(dataset),\n",
    "        \"n_samples\": 1,\n",
    "        \"description\": \"Temperature 0.3\",\n",
    "    },\n",
    "    \"temp_0.7\": {\n",
    "        \"sampler\": StandardSampler(temperature=0.7, top_p=0.95, max_tokens=1024),\n",
    "        \"evaluator\": AccuracyEvaluator(dataset),\n",
    "        \"n_samples\": 1,\n",
    "        \"description\": \"Temperature 0.7\",\n",
    "    },\n",
    "    \"self_consistency_5\": {\n",
    "        \"sampler\": StandardSampler(temperature=0.7, top_p=0.95, max_tokens=1024),\n",
    "        \"evaluator\": MajorityVotingEvaluator(dataset),\n",
    "        \"n_samples\": 5,\n",
    "        \"description\": \"Self-consistency (5 samples)\",\n",
    "    },\n",
    "    \"diverse_5\": {\n",
    "        \"sampler\": DiverseSampler(\n",
    "            temperatures=[0.3, 0.5, 0.7, 0.9],\n",
    "            top_p=0.95,\n",
    "            max_tokens=1024\n",
    "        ),\n",
    "        \"evaluator\": MajorityVotingEvaluator(dataset),\n",
    "        \"n_samples\": 5,\n",
    "        \"description\": \"Diverse temperatures (5 samples)\",\n",
    "    },\n",
    "}\n",
    "\n",
    "print(f\"Defined {len(experiments)} experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "\n",
    "for exp_name, exp_config in experiments.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running: {exp_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    results, metrics, responses, scores = run_evaluation(\n",
    "        model=model,\n",
    "        sampler=exp_config[\"sampler\"],\n",
    "        dataset=dataset,\n",
    "        evaluator=exp_config[\"evaluator\"],\n",
    "        batch_size=4,\n",
    "        n_samples=exp_config[\"n_samples\"],\n",
    "        max_problems=MAX_PROBLEMS,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    elapsed = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    run_dir = save_results(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        run_name=exp_name,\n",
    "        results=results,\n",
    "        metrics=metrics,\n",
    "        config={\"experiment\": exp_name},\n",
    "        responses=responses,\n",
    "        scores=scores,\n",
    "    )\n",
    "    \n",
    "    all_results[exp_name] = {\n",
    "        \"metrics\": metrics,\n",
    "        \"results\": results,\n",
    "        \"responses\": responses,\n",
    "        \"elapsed\": elapsed,\n",
    "    }\n",
    "    \n",
    "    print(f\"Accuracy: {metrics.accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detailed Constraint Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze per-constraint performance for greedy\n",
    "if \"greedy\" in all_results:\n",
    "    results = all_results[\"greedy\"][\"results\"]\n",
    "    responses = all_results[\"greedy\"][\"responses\"]\n",
    "    \n",
    "    constraint_success = defaultdict(list)\n",
    "    \n",
    "    for i, (problem, resps) in enumerate(zip(problems, responses)):\n",
    "        if not resps:\n",
    "            continue\n",
    "        response = resps[0]\n",
    "        \n",
    "        # Check each constraint individually\n",
    "        detailed = dataset.check_answer_detailed(response, problem.gold_answer)\n",
    "        \n",
    "        for ctype, passed in detailed.items():\n",
    "            base_type = ctype.split(':')[0] if ':' in ctype else ctype\n",
    "            constraint_success[base_type].append(1 if passed else 0)\n",
    "    \n",
    "    # Calculate success rates\n",
    "    constraint_rates = {}\n",
    "    for ctype, successes in constraint_success.items():\n",
    "        constraint_rates[ctype] = np.mean(successes)\n",
    "    \n",
    "    # Plot\n",
    "    sorted_rates = sorted(constraint_rates.items(), key=lambda x: x[1])\n",
    "    types, rates = zip(*sorted_rates)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    colors = ['#e74c3c' if r < 0.5 else '#f39c12' if r < 0.7 else '#2ecc71' for r in rates]\n",
    "    ax.barh(range(len(types)), rates, color=colors)\n",
    "    ax.set_yticks(range(len(types)))\n",
    "    ax.set_yticklabels(types)\n",
    "    ax.set_xlabel('Success Rate')\n",
    "    ax.set_title('Constraint Success Rate by Type (Greedy)')\n",
    "    ax.axvline(x=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/constraint_success_rates.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nHardest constraints:\")\n",
    "    for ctype, rate in sorted_rates[:5]:\n",
    "        print(f\"  {ctype}: {rate:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Success rate vs number of constraints\n",
    "if \"greedy\" in all_results:\n",
    "    results = all_results[\"greedy\"][\"results\"]\n",
    "    \n",
    "    constraint_count_success = defaultdict(list)\n",
    "    \n",
    "    for result, problem in zip(results, problems):\n",
    "        n_constraints = len(problem.gold_answer)\n",
    "        constraint_count_success[n_constraints].append(1 if result.correct else 0)\n",
    "    \n",
    "    # Plot\n",
    "    counts = sorted(constraint_count_success.keys())\n",
    "    success_rates = [np.mean(constraint_count_success[c]) for c in counts]\n",
    "    sample_sizes = [len(constraint_count_success[c]) for c in counts]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    bars = ax.bar(counts, success_rates, color='steelblue')\n",
    "    ax.set_xlabel('Number of Constraints')\n",
    "    ax.set_ylabel('Success Rate (All Constraints Met)')\n",
    "    ax.set_title('Success Rate vs Number of Constraints')\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # Add sample sizes\n",
    "    for bar, n in zip(bars, sample_sizes):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                f'n={n}', ha='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/success_vs_constraints.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table\n",
    "comparison_data = []\n",
    "for exp_name, data in all_results.items():\n",
    "    metrics = data[\"metrics\"]\n",
    "    comparison_data.append({\n",
    "        \"Experiment\": exp_name,\n",
    "        \"Description\": experiments[exp_name][\"description\"],\n",
    "        \"Accuracy\": metrics.accuracy,\n",
    "        \"Correct\": metrics.correct,\n",
    "        \"Total\": metrics.total,\n",
    "        \"Time (s)\": data[\"elapsed\"],\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "df = df.sort_values(\"Accuracy\", ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(df)))\n",
    "bars = ax.barh(df[\"Experiment\"], df[\"Accuracy\"], color=colors)\n",
    "ax.set_xlabel(\"Accuracy (All Constraints Met)\")\n",
    "ax.set_title(\"IFEval: Strategy Comparison\")\n",
    "ax.set_xlim(0, 1)\n",
    "\n",
    "for bar, acc in zip(bars, df[\"Accuracy\"]):\n",
    "    ax.text(acc + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "            f'{acc:.3f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/strategy_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find commonly failed problems\n",
    "if \"greedy\" in all_results:\n",
    "    failed_problems = []\n",
    "    results = all_results[\"greedy\"][\"results\"]\n",
    "    responses = all_results[\"greedy\"][\"responses\"]\n",
    "    \n",
    "    for result, problem, resps in zip(results, problems, responses):\n",
    "        if not result.correct and resps:\n",
    "            # Get which constraints failed\n",
    "            detailed = dataset.check_answer_detailed(resps[0], problem.gold_answer)\n",
    "            failed = [k for k, v in detailed.items() if not v]\n",
    "            failed_problems.append({\n",
    "                \"id\": problem.id,\n",
    "                \"prompt\": problem.prompt[:100],\n",
    "                \"failed_constraints\": failed,\n",
    "                \"total_constraints\": len(problem.gold_answer),\n",
    "            })\n",
    "    \n",
    "    print(f\"Failed problems: {len(failed_problems)} / {len(results)}\")\n",
    "    print(\"\\nExample failures:\")\n",
    "    for fp in failed_problems[:3]:\n",
    "        print(f\"\\n--- {fp['id']} ---\")\n",
    "        print(f\"Prompt: {fp['prompt']}...\")\n",
    "        print(f\"Failed: {fp['failed_constraints']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: IFEval Instruction Following\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(df[[\"Experiment\", \"Accuracy\", \"Correct\", \"Total\"]].to_string(index=False))\n",
    "\n",
    "print(f\"\\n\\nKey Findings:\")\n",
    "best_exp = df.iloc[0]\n",
    "print(f\"  1. Best accuracy: {best_exp['Experiment']} ({best_exp['Accuracy']:.4f})\")\n",
    "print(f\"  2. Greedy baseline: {all_results.get('greedy', {}).get('metrics', {}).accuracy if 'greedy' in all_results else 'N/A'}\")\n",
    "\n",
    "if 'constraint_rates' in dir():\n",
    "    hardest = min(constraint_rates.items(), key=lambda x: x[1])\n",
    "    easiest = max(constraint_rates.items(), key=lambda x: x[1])\n",
    "    print(f\"  3. Hardest constraint: {hardest[0]} ({hardest[1]:.3f})\")\n",
    "    print(f\"  4. Easiest constraint: {easiest[0]} ({easiest[1]:.3f})\")\n",
    "\n",
    "df.to_csv(f\"{OUTPUT_DIR}/summary.csv\", index=False)\n",
    "print(f\"\\nResults saved to {OUTPUT_DIR}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
