{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIME: Competition Math Reasoning Strategies\n",
    "\n",
    "This notebook compares different strategies on AIME (American Invitational Mathematics Examination) problems.\n",
    "\n",
    "AIME is significantly harder than GSM8K:\n",
    "- Competition-level problems\n",
    "- Requires deeper reasoning\n",
    "- Answers are integers 0-999\n",
    "\n",
    "**Strategies compared:**\n",
    "- Greedy decoding\n",
    "- Self-consistency (majority voting)\n",
    "- Tree search methods (Best-first, MCTS)\n",
    "- Extended thinking (more tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.models import VLLMModel\n",
    "from src.datasets import AIMEDataset\n",
    "from src.samplers import (\n",
    "    GreedySampler, StandardSampler, DiverseSampler,\n",
    "    BestFirstTreeSearch, MCTSTreeSearch\n",
    ")\n",
    "from src.evaluators import AccuracyEvaluator, MajorityVotingEvaluator\n",
    "from src.runners import run_evaluation\n",
    "from src.utils import save_results\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "MAX_PROBLEMS = 30  # AIME has ~450 problems total, but they're hard\n",
    "OUTPUT_DIR = \"../results/aime_comparison\"\n",
    "\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-10 02:38:56 [importing.py:44] Triton is installed but 0 active driver(s) found (expected 1). Disabling Triton to prevent runtime errors.\n",
      "INFO 01-10 02:38:56 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.\n",
      "INFO 01-10 02:39:00 [utils.py:253] non-default args: {'trust_remote_code': True, 'seed': 42, 'disable_log_stats': True, 'model': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Device string must not be empty",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model = \u001b[43mVLLMModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel loaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/reasoning-language-models/src/models/local.py:45\u001b[39m, in \u001b[36mVLLMModel.__init__\u001b[39m\u001b[34m(self, model_name, dtype, gpu_memory_utilization, max_model_len, tensor_parallel_size, seed)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28mself\u001b[39m._seed = seed\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Initialize vLLM engine\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[38;5;28mself\u001b[39m.llm = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer = \u001b[38;5;28mself\u001b[39m.llm.get_tokenizer()\n\u001b[32m     56\u001b[39m \u001b[38;5;28mself\u001b[39m.SamplingParams = SamplingParams\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/vllm/entrypoints/llm.py:351\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, runner, convert, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, allowed_media_domains, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, disable_custom_all_reduce, hf_token, hf_overrides, mm_processor_kwargs, pooler_config, structured_outputs_config, profiler_config, kv_cache_memory_bytes, compilation_config, logits_processors, **kwargs)\u001b[39m\n\u001b[32m    316\u001b[39m engine_args = EngineArgs(\n\u001b[32m    317\u001b[39m     model=model,\n\u001b[32m    318\u001b[39m     runner=runner,\n\u001b[32m   (...)\u001b[39m\u001b[32m    346\u001b[39m     **kwargs,\n\u001b[32m    347\u001b[39m )\n\u001b[32m    349\u001b[39m log_non_default_args(engine_args)\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n\u001b[32m    356\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:175\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers, enable_multiprocessing)\u001b[39m\n\u001b[32m    172\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Creates an LLM engine from the engine arguments.\"\"\"\u001b[39;00m\n\u001b[32m    174\u001b[39m \u001b[38;5;66;03m# Create the engine configs.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m vllm_config = \u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_engine_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m executor_class = Executor.get_class(vllm_config)\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m envs.VLLM_ENABLE_V1_MULTIPROCESSING:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/vllm/engine/arg_utils.py:1314\u001b[39m, in \u001b[36mEngineArgs.create_engine_config\u001b[39m\u001b[34m(self, usage_context, headless)\u001b[39m\n\u001b[32m   1307\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1308\u001b[39m \u001b[33;03mCreate the VllmConfig.\u001b[39;00m\n\u001b[32m   1309\u001b[39m \n\u001b[32m   1310\u001b[39m \u001b[33;03mNOTE: If VllmConfig is incompatible, we raise an error.\u001b[39;00m\n\u001b[32m   1311\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1312\u001b[39m current_platform.pre_register_and_update()\n\u001b[32m-> \u001b[39m\u001b[32m1314\u001b[39m device_config = \u001b[43mDeviceConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_platform\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1316\u001b[39m \u001b[38;5;66;03m# Check if the model is a speculator and override model/tokenizer/config\u001b[39;00m\n\u001b[32m   1317\u001b[39m \u001b[38;5;66;03m# BEFORE creating ModelConfig, so the config is created with the target model\u001b[39;00m\n\u001b[32m   1318\u001b[39m \u001b[38;5;66;03m# Skip speculator detection for cloud storage models (eg: S3, GCS) since\u001b[39;00m\n\u001b[32m   1319\u001b[39m \u001b[38;5;66;03m# HuggingFace cannot load configs directly from S3 URLs. S3 models can still\u001b[39;00m\n\u001b[32m   1320\u001b[39m \u001b[38;5;66;03m# use speculators with explicit --speculative-config.\u001b[39;00m\n\u001b[32m   1321\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cloud_storage(\u001b[38;5;28mself\u001b[39m.model):\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/vllm/config/device.py:75\u001b[39m, in \u001b[36mDeviceConfig.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     72\u001b[39m     \u001b[38;5;28mself\u001b[39m.device = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     74\u001b[39m     \u001b[38;5;66;03m# Set device with device type\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     \u001b[38;5;28mself\u001b[39m.device = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Device string must not be empty"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "print(\"Loading model...\")\n",
    "model = VLLMModel(\n",
    "    model_name=MODEL_NAME,\n",
    "    gpu_memory_utilization=0.9,\n",
    "    dtype=\"auto\",\n",
    ")\n",
    "print(f\"Model loaded: {model.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = AIMEDataset(source=\"hf\")\n",
    "print(f\"Dataset: {dataset.name}, {len(dataset)} problems\")\n",
    "\n",
    "problems = dataset.get_problems(limit=MAX_PROBLEMS)\n",
    "print(f\"\\nUsing {len(problems)} problems for evaluation\")\n",
    "\n",
    "# Preview\n",
    "print(f\"\\nExample problem:\")\n",
    "print(f\"Prompt: {problems[0].prompt[:300]}...\")\n",
    "print(f\"Gold answer: {problems[0].gold_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Experiments\n",
    "\n",
    "For AIME, we expect tree search methods to be more beneficial due to problem difficulty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = {\n",
    "    # Baseline\n",
    "    \"greedy\": {\n",
    "        \"sampler\": GreedySampler(max_tokens=2048),\n",
    "        \"evaluator\": AccuracyEvaluator(dataset),\n",
    "        \"n_samples\": 1,\n",
    "        \"description\": \"Greedy decoding\",\n",
    "    },\n",
    "    \n",
    "    # Extended thinking (more tokens)\n",
    "    \"greedy_long\": {\n",
    "        \"sampler\": GreedySampler(max_tokens=4096),\n",
    "        \"evaluator\": AccuracyEvaluator(dataset),\n",
    "        \"n_samples\": 1,\n",
    "        \"description\": \"Greedy with extended tokens (4096)\",\n",
    "    },\n",
    "    \n",
    "    # Self-consistency\n",
    "    \"self_consistency_8\": {\n",
    "        \"sampler\": StandardSampler(temperature=0.8, top_p=0.95, max_tokens=2048),\n",
    "        \"evaluator\": MajorityVotingEvaluator(dataset),\n",
    "        \"n_samples\": 8,\n",
    "        \"description\": \"Self-consistency (8 samples)\",\n",
    "    },\n",
    "    \n",
    "    # Higher temperature self-consistency (more exploration)\n",
    "    \"self_consistency_high_temp\": {\n",
    "        \"sampler\": StandardSampler(temperature=1.0, top_p=0.95, max_tokens=2048),\n",
    "        \"evaluator\": MajorityVotingEvaluator(dataset),\n",
    "        \"n_samples\": 8,\n",
    "        \"description\": \"Self-consistency (8 samples, temp=1.0)\",\n",
    "    },\n",
    "    \n",
    "    # Diverse sampling\n",
    "    \"diverse_16\": {\n",
    "        \"sampler\": DiverseSampler(\n",
    "            temperatures=[0.5, 0.7, 0.9, 1.0, 1.2],\n",
    "            top_p=0.95,\n",
    "            max_tokens=2048\n",
    "        ),\n",
    "        \"evaluator\": MajorityVotingEvaluator(dataset),\n",
    "        \"n_samples\": 16,\n",
    "        \"description\": \"Diverse temperatures (16 samples)\",\n",
    "    },\n",
    "    \n",
    "    # Best-first tree search (smaller budget)\n",
    "    \"best_first_small\": {\n",
    "        \"sampler\": BestFirstTreeSearch(\n",
    "            max_expansions=20,\n",
    "            branch_factor=3,\n",
    "            max_tokens=512,\n",
    "            tokens_per_step=64,\n",
    "            temperature=0.8,\n",
    "        ),\n",
    "        \"evaluator\": AccuracyEvaluator(dataset),\n",
    "        \"n_samples\": 1,\n",
    "        \"description\": \"Best-first tree (20 expansions)\",\n",
    "    },\n",
    "    \n",
    "    # Best-first tree search (larger budget)\n",
    "    \"best_first_large\": {\n",
    "        \"sampler\": BestFirstTreeSearch(\n",
    "            max_expansions=50,\n",
    "            branch_factor=4,\n",
    "            max_tokens=768,\n",
    "            tokens_per_step=48,\n",
    "            temperature=0.8,\n",
    "        ),\n",
    "        \"evaluator\": AccuracyEvaluator(dataset),\n",
    "        \"n_samples\": 1,\n",
    "        \"description\": \"Best-first tree (50 expansions)\",\n",
    "    },\n",
    "    \n",
    "    # MCTS (smaller budget)\n",
    "    \"mcts_small\": {\n",
    "        \"sampler\": MCTSTreeSearch(\n",
    "            max_iterations=30,\n",
    "            branch_factor=3,\n",
    "            max_tokens=512,\n",
    "            tokens_per_step=64,\n",
    "            rollout_tokens=128,\n",
    "            temperature=0.8,\n",
    "            exploration_constant=1.5,\n",
    "        ),\n",
    "        \"evaluator\": AccuracyEvaluator(dataset),\n",
    "        \"n_samples\": 1,\n",
    "        \"description\": \"MCTS (30 iterations)\",\n",
    "    },\n",
    "    \n",
    "    # MCTS (larger budget, more exploration)\n",
    "    \"mcts_large\": {\n",
    "        \"sampler\": MCTSTreeSearch(\n",
    "            max_iterations=80,\n",
    "            branch_factor=4,\n",
    "            max_tokens=768,\n",
    "            tokens_per_step=48,\n",
    "            rollout_tokens=128,\n",
    "            temperature=0.8,\n",
    "            exploration_constant=2.0,  # More exploration\n",
    "        ),\n",
    "        \"evaluator\": AccuracyEvaluator(dataset),\n",
    "        \"n_samples\": 1,\n",
    "        \"description\": \"MCTS (80 iterations, high exploration)\",\n",
    "    },\n",
    "}\n",
    "\n",
    "print(f\"Defined {len(experiments)} experiments:\")\n",
    "for name, exp in experiments.items():\n",
    "    print(f\"  - {name}: {exp['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "\n",
    "for exp_name, exp_config in experiments.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running: {exp_name}\")\n",
    "    print(f\"Description: {exp_config['description']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Tree search methods need batch_size=1\n",
    "    is_tree = \"tree\" in exp_name or \"mcts\" in exp_name\n",
    "    batch_size = 1 if is_tree else (2 if exp_config[\"n_samples\"] > 1 else 4)\n",
    "    \n",
    "    results, metrics, responses, scores = run_evaluation(\n",
    "        model=model,\n",
    "        sampler=exp_config[\"sampler\"],\n",
    "        dataset=dataset,\n",
    "        evaluator=exp_config[\"evaluator\"],\n",
    "        batch_size=batch_size,\n",
    "        n_samples=exp_config[\"n_samples\"],\n",
    "        max_problems=MAX_PROBLEMS,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    elapsed = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    run_dir = save_results(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        run_name=exp_name,\n",
    "        results=results,\n",
    "        metrics=metrics,\n",
    "        config={\n",
    "            \"experiment\": exp_name,\n",
    "            \"description\": exp_config[\"description\"],\n",
    "            \"n_samples\": exp_config[\"n_samples\"],\n",
    "            \"model\": MODEL_NAME,\n",
    "        },\n",
    "        responses=responses,\n",
    "        scores=scores,\n",
    "    )\n",
    "    \n",
    "    all_results[exp_name] = {\n",
    "        \"metrics\": metrics,\n",
    "        \"results\": results,\n",
    "        \"elapsed\": elapsed,\n",
    "        \"n_samples\": exp_config[\"n_samples\"],\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nCompleted in {elapsed:.1f}s\")\n",
    "    print(f\"Accuracy: {metrics.accuracy:.4f} ({metrics.correct}/{metrics.total})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "for exp_name, data in all_results.items():\n",
    "    metrics = data[\"metrics\"]\n",
    "    comparison_data.append({\n",
    "        \"Experiment\": exp_name,\n",
    "        \"Description\": experiments[exp_name][\"description\"],\n",
    "        \"Accuracy\": metrics.accuracy,\n",
    "        \"Correct\": metrics.correct,\n",
    "        \"Total\": metrics.total,\n",
    "        \"N Samples\": data[\"n_samples\"],\n",
    "        \"Time (s)\": data[\"elapsed\"],\n",
    "        \"Time/Problem (s)\": data[\"elapsed\"] / metrics.total,\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "df = df.sort_values(\"Accuracy\", ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Accuracy comparison\n",
    "ax1 = axes[0]\n",
    "colors = ['#2ecc71' if 'mcts' in name or 'tree' in name else \n",
    "          '#3498db' if 'consistency' in name or 'diverse' in name else \n",
    "          '#e74c3c' for name in df[\"Experiment\"]]\n",
    "bars = ax1.barh(df[\"Experiment\"], df[\"Accuracy\"], color=colors)\n",
    "ax1.set_xlabel(\"Accuracy\")\n",
    "ax1.set_title(\"AIME Accuracy by Strategy\")\n",
    "ax1.set_xlim(0, max(df[\"Accuracy\"]) * 1.2 if df[\"Accuracy\"].max() > 0 else 0.3)\n",
    "\n",
    "for bar, acc in zip(bars, df[\"Accuracy\"]):\n",
    "    ax1.text(acc + 0.005, bar.get_y() + bar.get_height()/2, \n",
    "             f'{acc:.3f}', va='center', fontsize=10)\n",
    "\n",
    "# Time comparison\n",
    "ax2 = axes[1]\n",
    "bars = ax2.barh(df[\"Experiment\"], df[\"Time/Problem (s)\"], color=colors)\n",
    "ax2.set_xlabel(\"Time per Problem (seconds)\")\n",
    "ax2.set_title(\"Compute Time by Strategy\")\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#2ecc71', label='Tree Search'),\n",
    "    Patch(facecolor='#3498db', label='Self-Consistency'),\n",
    "    Patch(facecolor='#e74c3c', label='Greedy'),\n",
    "]\n",
    "ax1.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/aime_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy vs Time scatter\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "# Group by method type\n",
    "for exp_name, data in all_results.items():\n",
    "    metrics = data[\"metrics\"]\n",
    "    \n",
    "    if 'mcts' in exp_name:\n",
    "        color, marker = '#2ecc71', 's'  # Green square\n",
    "    elif 'tree' in exp_name:\n",
    "        color, marker = '#27ae60', '^'  # Darker green triangle\n",
    "    elif 'consistency' in exp_name or 'diverse' in exp_name:\n",
    "        color, marker = '#3498db', 'o'  # Blue circle\n",
    "    else:\n",
    "        color, marker = '#e74c3c', 'D'  # Red diamond\n",
    "    \n",
    "    ax.scatter(\n",
    "        data[\"elapsed\"] / metrics.total,\n",
    "        metrics.accuracy,\n",
    "        s=150,\n",
    "        c=color,\n",
    "        marker=marker,\n",
    "        alpha=0.8,\n",
    "        edgecolors='black',\n",
    "        linewidth=1,\n",
    "    )\n",
    "    ax.annotate(\n",
    "        exp_name.replace('_', '\\n'),\n",
    "        (data[\"elapsed\"] / metrics.total, metrics.accuracy),\n",
    "        xytext=(8, 0),\n",
    "        textcoords='offset points',\n",
    "        fontsize=8,\n",
    "        va='center',\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"Time per Problem (seconds)\", fontsize=12)\n",
    "ax.set_ylabel(\"Accuracy\", fontsize=12)\n",
    "ax.set_title(\"AIME: Accuracy vs Compute Tradeoff\", fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/aime_pareto.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Problem Difficulty Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze which problems each method solves\n",
    "method_correct = {}\n",
    "for exp_name, data in all_results.items():\n",
    "    method_correct[exp_name] = {r.problem_id: r.correct for r in data[\"results\"]}\n",
    "\n",
    "# Count how many methods solve each problem\n",
    "problem_solve_counts = {}\n",
    "for pid in method_correct[\"greedy\"].keys():\n",
    "    count = sum(1 for method in method_correct.values() if method.get(pid, False))\n",
    "    problem_solve_counts[pid] = count\n",
    "\n",
    "# Distribution\n",
    "counts = list(problem_solve_counts.values())\n",
    "n_methods = len(all_results)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist(counts, bins=range(n_methods + 2), align='left', rwidth=0.8, color='steelblue')\n",
    "ax.set_xlabel(\"Number of Methods that Solved Problem\")\n",
    "ax.set_ylabel(\"Number of Problems\")\n",
    "ax.set_title(\"Problem Difficulty Distribution\")\n",
    "ax.set_xticks(range(n_methods + 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/problem_difficulty.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Problems solved by ALL methods: {sum(1 for c in counts if c == n_methods)}\")\n",
    "print(f\"Problems solved by NO method: {sum(1 for c in counts if c == 0)}\")\n",
    "print(f\"Problems solved by SOME methods: {sum(1 for c in counts if 0 < c < n_methods)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find problems uniquely solved by tree search\n",
    "tree_methods = [name for name in all_results.keys() if 'mcts' in name or 'tree' in name]\n",
    "other_methods = [name for name in all_results.keys() if name not in tree_methods]\n",
    "\n",
    "unique_tree_solves = []\n",
    "for pid in method_correct[\"greedy\"].keys():\n",
    "    tree_solved = any(method_correct[m].get(pid, False) for m in tree_methods)\n",
    "    other_solved = any(method_correct[m].get(pid, False) for m in other_methods)\n",
    "    if tree_solved and not other_solved:\n",
    "        unique_tree_solves.append(pid)\n",
    "\n",
    "print(f\"\\nProblems uniquely solved by tree search methods: {len(unique_tree_solves)}\")\n",
    "\n",
    "# Show examples\n",
    "if unique_tree_solves:\n",
    "    print(\"\\nExample problems uniquely solved by tree search:\")\n",
    "    for pid in unique_tree_solves[:2]:\n",
    "        for p in problems:\n",
    "            if p.id == pid:\n",
    "                print(f\"\\n--- {pid} ---\")\n",
    "                print(f\"Question: {p.prompt[:400]}...\")\n",
    "                print(f\"Gold answer: {p.gold_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: AIME Strategy Comparison\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_df = df[[\"Experiment\", \"Accuracy\", \"Correct\", \"Total\", \"Time/Problem (s)\"]].copy()\n",
    "summary_df[\"Accuracy\"] = summary_df[\"Accuracy\"].apply(lambda x: f\"{x:.4f}\")\n",
    "summary_df[\"Time/Problem (s)\"] = summary_df[\"Time/Problem (s)\"].apply(lambda x: f\"{x:.2f}\")\n",
    "\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Analysis\n",
    "best_exp = df.iloc[0]\n",
    "greedy_acc = all_results[\"greedy\"][\"metrics\"].accuracy\n",
    "\n",
    "print(f\"\\n\\nKey Findings:\")\n",
    "print(f\"  1. Best accuracy: {best_exp['Experiment']} ({best_exp['Accuracy']})\")\n",
    "print(f\"  2. Greedy baseline: {greedy_acc:.4f}\")\n",
    "print(f\"  3. Best improvement over greedy: +{float(best_exp['Accuracy']) - greedy_acc:.4f}\")\n",
    "\n",
    "# Tree search analysis\n",
    "tree_accs = [all_results[m][\"metrics\"].accuracy for m in tree_methods if m in all_results]\n",
    "other_accs = [all_results[m][\"metrics\"].accuracy for m in other_methods if m in all_results]\n",
    "\n",
    "if tree_accs and other_accs:\n",
    "    print(f\"\\n  Tree search methods avg accuracy: {np.mean(tree_accs):.4f}\")\n",
    "    print(f\"  Other methods avg accuracy: {np.mean(other_accs):.4f}\")\n",
    "\n",
    "df.to_csv(f\"{OUTPUT_DIR}/summary.csv\", index=False)\n",
    "print(f\"\\nResults saved to {OUTPUT_DIR}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
