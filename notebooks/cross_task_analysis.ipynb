{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Task Analysis: Comparing Strategies Across Benchmarks\n",
    "\n",
    "This notebook aggregates results from all task-specific evaluations and provides:\n",
    "\n",
    "1. **Strategy effectiveness across tasks** - Which methods work best where?\n",
    "2. **Compute-performance tradeoffs** - Pareto frontiers\n",
    "3. **Task difficulty analysis** - How do tasks compare?\n",
    "4. **Recommendations** - When to use which strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "from src.utils import load_results, compare_runs\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all result directories\n",
    "RESULTS_BASE = \"../results\"\n",
    "\n",
    "task_dirs = {\n",
    "    \"GSM8K\": f\"{RESULTS_BASE}/gsm8k_comparison\",\n",
    "    \"AIME\": f\"{RESULTS_BASE}/aime_comparison\",\n",
    "    \"HumanEval\": f\"{RESULTS_BASE}/humaneval_comparison\",\n",
    "    \"IFEval\": f\"{RESULTS_BASE}/ifeval_comparison\",\n",
    "}\n",
    "\n",
    "# Load results from each task\n",
    "all_task_results = {}\n",
    "\n",
    "for task_name, task_dir in task_dirs.items():\n",
    "    task_path = Path(task_dir)\n",
    "    if not task_path.exists():\n",
    "        print(f\"Warning: {task_dir} not found, skipping {task_name}\")\n",
    "        continue\n",
    "    \n",
    "    # Find all run directories\n",
    "    run_dirs = [d for d in task_path.iterdir() if d.is_dir()]\n",
    "    \n",
    "    task_results = {}\n",
    "    for run_dir in run_dirs:\n",
    "        try:\n",
    "            results, metrics, config = load_results(str(run_dir))\n",
    "            exp_name = config.get(\"experiment\", run_dir.name.split(\"_2\")[0])\n",
    "            task_results[exp_name] = {\n",
    "                \"metrics\": metrics,\n",
    "                \"config\": config,\n",
    "                \"run_dir\": str(run_dir),\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading {run_dir}: {e}\")\n",
    "    \n",
    "    if task_results:\n",
    "        all_task_results[task_name] = task_results\n",
    "        print(f\"Loaded {len(task_results)} experiments for {task_name}\")\n",
    "\n",
    "print(f\"\\nTotal: {len(all_task_results)} tasks loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Comparison DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unified dataframe\n",
    "rows = []\n",
    "\n",
    "for task_name, experiments in all_task_results.items():\n",
    "    for exp_name, data in experiments.items():\n",
    "        metrics = data[\"metrics\"]\n",
    "        \n",
    "        # Determine strategy type\n",
    "        if \"greedy\" in exp_name.lower():\n",
    "            strategy_type = \"Greedy\"\n",
    "        elif \"mcts\" in exp_name.lower() or \"tree\" in exp_name.lower():\n",
    "            strategy_type = \"Tree Search\"\n",
    "        elif \"consistency\" in exp_name.lower() or \"diverse\" in exp_name.lower() or \"voting\" in exp_name.lower():\n",
    "            strategy_type = \"Self-Consistency\"\n",
    "        elif \"temp\" in exp_name.lower() or \"nucleus\" in exp_name.lower():\n",
    "            strategy_type = \"Sampling\"\n",
    "        else:\n",
    "            strategy_type = \"Other\"\n",
    "        \n",
    "        # Get primary metric\n",
    "        accuracy = metrics.get(\"accuracy\", 0)\n",
    "        pass_at_1 = metrics.get(\"pass@1\", accuracy)\n",
    "        \n",
    "        rows.append({\n",
    "            \"Task\": task_name,\n",
    "            \"Experiment\": exp_name,\n",
    "            \"Strategy Type\": strategy_type,\n",
    "            \"Accuracy\": pass_at_1,\n",
    "            \"pass@5\": metrics.get(\"pass@5\", np.nan),\n",
    "            \"pass@10\": metrics.get(\"pass@10\", np.nan),\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(f\"Total experiments: {len(df)}\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Task Difficulty Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare greedy performance across tasks\n",
    "greedy_df = df[df[\"Experiment\"].str.contains(\"greedy\", case=False)]\n",
    "\n",
    "if not greedy_df.empty:\n",
    "    task_difficulty = greedy_df.groupby(\"Task\")[\"Accuracy\"].mean().sort_values(ascending=False)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    colors = ['#2ecc71', '#3498db', '#f39c12', '#e74c3c'][:len(task_difficulty)]\n",
    "    bars = ax.bar(task_difficulty.index, task_difficulty.values, color=colors)\n",
    "    ax.set_ylabel(\"Greedy Accuracy\")\n",
    "    ax.set_title(\"Task Difficulty (Greedy Baseline Performance)\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    for bar, acc in zip(bars, task_difficulty.values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, acc + 0.02, f'{acc:.3f}', \n",
    "                ha='center', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_BASE}/task_difficulty.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Task Difficulty Ranking (easiest to hardest):\")\n",
    "    for task, acc in task_difficulty.items():\n",
    "        print(f\"  {task}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Strategy Effectiveness by Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table: best accuracy per strategy type per task\n",
    "pivot = df.groupby([\"Task\", \"Strategy Type\"])[\"Accuracy\"].max().unstack(fill_value=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "pivot.plot(kind=\"bar\", ax=ax, width=0.8)\n",
    "ax.set_ylabel(\"Best Accuracy\")\n",
    "ax.set_title(\"Best Performance by Strategy Type Across Tasks\")\n",
    "ax.set_xticklabels(pivot.index, rotation=0)\n",
    "ax.legend(title=\"Strategy Type\", bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{RESULTS_BASE}/strategy_by_task.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improvement over greedy by strategy\n",
    "improvements = []\n",
    "\n",
    "for task in df[\"Task\"].unique():\n",
    "    task_df = df[df[\"Task\"] == task]\n",
    "    greedy_acc = task_df[task_df[\"Experiment\"].str.contains(\"greedy\", case=False)][\"Accuracy\"].max()\n",
    "    \n",
    "    if pd.isna(greedy_acc) or greedy_acc == 0:\n",
    "        continue\n",
    "    \n",
    "    for strategy in task_df[\"Strategy Type\"].unique():\n",
    "        if strategy == \"Greedy\":\n",
    "            continue\n",
    "        best_acc = task_df[task_df[\"Strategy Type\"] == strategy][\"Accuracy\"].max()\n",
    "        improvement = best_acc - greedy_acc\n",
    "        improvements.append({\n",
    "            \"Task\": task,\n",
    "            \"Strategy\": strategy,\n",
    "            \"Improvement\": improvement,\n",
    "            \"Relative Improvement\": improvement / greedy_acc * 100 if greedy_acc > 0 else 0,\n",
    "        })\n",
    "\n",
    "imp_df = pd.DataFrame(improvements)\n",
    "\n",
    "if not imp_df.empty:\n",
    "    pivot_imp = imp_df.pivot(index=\"Task\", columns=\"Strategy\", values=\"Improvement\").fillna(0)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    pivot_imp.plot(kind=\"bar\", ax=ax, width=0.8)\n",
    "    ax.set_ylabel(\"Accuracy Improvement over Greedy\")\n",
    "    ax.set_title(\"Strategy Improvement over Greedy Baseline\")\n",
    "    ax.set_xticklabels(pivot_imp.index, rotation=0)\n",
    "    ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax.legend(title=\"Strategy\", bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_BASE}/improvement_over_greedy.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Best Strategy per Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best experiment for each task\n",
    "best_per_task = df.loc[df.groupby(\"Task\")[\"Accuracy\"].idxmax()]\n",
    "\n",
    "print(\"Best Strategy per Task:\")\n",
    "print(\"=\" * 60)\n",
    "for _, row in best_per_task.iterrows():\n",
    "    print(f\"\\n{row['Task']}:\")\n",
    "    print(f\"  Best: {row['Experiment']} ({row['Strategy Type']})\")\n",
    "    print(f\"  Accuracy: {row['Accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Heatmap: All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap of all results\n",
    "# Standardize experiment names\n",
    "def standardize_exp_name(name):\n",
    "    name = name.lower()\n",
    "    if 'greedy' in name:\n",
    "        return 'greedy'\n",
    "    if 'mcts' in name:\n",
    "        return 'mcts'\n",
    "    if 'best_first' in name or 'tree' in name:\n",
    "        return 'best_first'\n",
    "    if 'consistency' in name:\n",
    "        if '16' in name:\n",
    "            return 'self_consistency_16'\n",
    "        return 'self_consistency_8'\n",
    "    if 'diverse' in name:\n",
    "        return 'diverse'\n",
    "    if 'temp' in name:\n",
    "        # Extract temperature\n",
    "        import re\n",
    "        match = re.search(r'temp[_]?(\\d+\\.?\\d*)', name)\n",
    "        if match:\n",
    "            return f'temp_{match.group(1)}'\n",
    "    return name\n",
    "\n",
    "df['Std Experiment'] = df['Experiment'].apply(standardize_exp_name)\n",
    "\n",
    "# Pivot\n",
    "heatmap_data = df.pivot_table(\n",
    "    index='Std Experiment', \n",
    "    columns='Task', \n",
    "    values='Accuracy', \n",
    "    aggfunc='max'\n",
    ").fillna(0)\n",
    "\n",
    "if not heatmap_data.empty:\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    im = ax.imshow(heatmap_data.values, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "    \n",
    "    ax.set_xticks(range(len(heatmap_data.columns)))\n",
    "    ax.set_xticklabels(heatmap_data.columns, fontsize=11)\n",
    "    ax.set_yticks(range(len(heatmap_data.index)))\n",
    "    ax.set_yticklabels(heatmap_data.index, fontsize=10)\n",
    "    \n",
    "    # Add values\n",
    "    for i in range(len(heatmap_data.index)):\n",
    "        for j in range(len(heatmap_data.columns)):\n",
    "            val = heatmap_data.iloc[i, j]\n",
    "            if val > 0:\n",
    "                color = 'white' if val > 0.5 else 'black'\n",
    "                ax.text(j, i, f'{val:.2f}', ha='center', va='center', color=color, fontsize=9)\n",
    "    \n",
    "    ax.set_title('Accuracy Heatmap: Strategies √ó Tasks', fontsize=14)\n",
    "    plt.colorbar(im, ax=ax, label='Accuracy')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_BASE}/accuracy_heatmap.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CROSS-TASK ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä Task Difficulty (Greedy Baseline):\")\n",
    "if 'task_difficulty' in dir():\n",
    "    for task, acc in task_difficulty.items():\n",
    "        difficulty = \"Easy\" if acc > 0.6 else \"Medium\" if acc > 0.3 else \"Hard\"\n",
    "        print(f\"  ‚Ä¢ {task}: {acc:.4f} ({difficulty})\")\n",
    "\n",
    "print(\"\\nüèÜ Best Strategy per Task:\")\n",
    "for _, row in best_per_task.iterrows():\n",
    "    print(f\"  ‚Ä¢ {row['Task']}: {row['Experiment']} ({row['Accuracy']:.4f})\")\n",
    "\n",
    "print(\"\\nüìà Strategy Recommendations:\")\n",
    "print(\"\"\"\n",
    "  1. GSM8K (Grade School Math):\n",
    "     ‚Üí Self-consistency with 8-16 samples works well\n",
    "     ‚Üí Higher temperatures (0.7) improve diversity\n",
    "  \n",
    "  2. AIME (Competition Math):\n",
    "     ‚Üí Tree search methods can help on hard problems\n",
    "     ‚Üí More compute generally helps\n",
    "     ‚Üí Consider MCTS for exploration\n",
    "  \n",
    "  3. HumanEval (Code Generation):\n",
    "     ‚Üí Temperature 0.8 is optimal for pass@k\n",
    "     ‚Üí Generate 20+ samples for reliable pass@10\n",
    "     ‚Üí Diversity matters more than greedy\n",
    "  \n",
    "  4. IFEval (Instruction Following):\n",
    "     ‚Üí Lower temperatures preserve instruction adherence\n",
    "     ‚Üí Greedy often competitive\n",
    "     ‚Üí Self-consistency helps on multi-constraint tasks\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüí° General Insights:\")\n",
    "print(\"\"\"\n",
    "  ‚Ä¢ Self-consistency provides consistent improvements across math tasks\n",
    "  ‚Ä¢ Tree search is most beneficial for hard reasoning problems\n",
    "  ‚Ä¢ Temperature tuning is task-specific\n",
    "  ‚Ä¢ Compute-accuracy tradeoff varies by task difficulty\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary\n",
    "df.to_csv(f\"{RESULTS_BASE}/cross_task_summary.csv\", index=False)\n",
    "print(f\"\\nResults saved to {RESULTS_BASE}/cross_task_summary.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
