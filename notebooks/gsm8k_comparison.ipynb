{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSM8K: Comparing Reasoning Strategies\n",
    "\n",
    "This notebook compares different sampling and evaluation strategies on the GSM8K (Grade School Math) benchmark.\n",
    "\n",
    "**Strategies compared:**\n",
    "- Greedy decoding\n",
    "- Temperature sampling with self-consistency\n",
    "- Best-of-N sampling\n",
    "- Tree search (Best-first & MCTS)\n",
    "\n",
    "**Metrics:**\n",
    "- Accuracy\n",
    "- Majority voting accuracy\n",
    "- Agreement rate\n",
    "- Compute efficiency (tokens/problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.models import load_model, VLLMModel\n",
    "from src.datasets import GSM8KDataset\n",
    "from src.samplers import (\n",
    "    GreedySampler, StandardSampler, NucleusSampler, \n",
    "    DiverseSampler, BestFirstTreeSearch, MCTSTreeSearch\n",
    ")\n",
    "from src.evaluators import (\n",
    "    AccuracyEvaluator, MajorityVotingEvaluator, \n",
    "    BestOfNEvaluator, WeightedVotingEvaluator\n",
    ")\n",
    "from src.runners import run_evaluation\n",
    "from src.utils import save_results, load_results, compare_runs\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "MAX_PROBLEMS = 200  # Set to None for full evaluation\n",
    "OUTPUT_DIR = \"../results/gsm8k_comparison\"\n",
    "\n",
    "# Create output directory\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model (this may take a minute)\n",
    "print(\"Loading model...\")\n",
    "model = VLLMModel(\n",
    "    model_name=MODEL_NAME,\n",
    "    gpu_memory_utilization=0.9,\n",
    "    dtype=\"auto\",\n",
    ")\n",
    "print(f\"Model loaded: {model.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = GSM8KDataset(split=\"test\", use_cot_prompt=True)\n",
    "print(f\"Dataset: {dataset.name}, {len(dataset)} problems\")\n",
    "\n",
    "# Preview a problem\n",
    "problems = dataset.get_problems(limit=MAX_PROBLEMS)\n",
    "print(f\"\\nUsing {len(problems)} problems for evaluation\")\n",
    "print(f\"\\nExample problem:\")\n",
    "print(f\"Prompt: {problems[0].prompt[:200]}...\")\n",
    "print(f\"Gold answer: {problems[0].gold_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Experiments\n",
    "\n",
    "We'll compare several strategies with different compute budgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experimental configurations\n",
    "experiments = {\n",
    "    # Baseline: Single greedy sample\n",
    "    \"greedy\": {\n",
    "        \"sampler\": GreedySampler(max_tokens=2048),\n",
    "        \"evaluator\": AccuracyEvaluator(dataset),\n",
    "        \"n_samples\": 1,\n",
    "        \"description\": \"Greedy decoding (temperature=0)\",\n",
    "    },\n",
    "    \n",
    "    # Temperature sampling\n",
    "    \"temp_0.7\": {\n",
    "        \"sampler\": StandardSampler(temperature=0.7, top_p=0.95, max_tokens=2048),\n",
    "        \"evaluator\": AccuracyEvaluator(dataset),\n",
    "        \"n_samples\": 1,\n",
    "        \"description\": \"Temperature=0.7 single sample\",\n",
    "    },\n",
    "    \n",
    "    # Self-consistency with 8 samples\n",
    "    \"self_consistency_8\": {\n",
    "        \"sampler\": StandardSampler(temperature=0.7, top_p=0.95, max_tokens=2048),\n",
    "        \"evaluator\": MajorityVotingEvaluator(dataset),\n",
    "        \"n_samples\": 8,\n",
    "        \"description\": \"Self-consistency (8 samples, majority vote)\",\n",
    "    },\n",
    "    \n",
    "    # Self-consistency with 16 samples\n",
    "    \"self_consistency_16\": {\n",
    "        \"sampler\": StandardSampler(temperature=0.7, top_p=0.95, max_tokens=2048),\n",
    "        \"evaluator\": MajorityVotingEvaluator(dataset),\n",
    "        \"n_samples\": 16,\n",
    "        \"description\": \"Self-consistency (16 samples, majority vote)\",\n",
    "    },\n",
    "    \n",
    "    # Diverse sampling with self-consistency\n",
    "    \"diverse_consistency_8\": {\n",
    "        \"sampler\": DiverseSampler(\n",
    "            temperatures=[0.3, 0.5, 0.7, 0.9, 1.0],\n",
    "            top_p=0.95,\n",
    "            max_tokens=2048\n",
    "        ),\n",
    "        \"evaluator\": MajorityVotingEvaluator(dataset),\n",
    "        \"n_samples\": 8,\n",
    "        \"description\": \"Diverse temperatures + majority vote (8 samples)\",\n",
    "    },\n",
    "    \n",
    "    # Best-of-N by log probability\n",
    "    \"best_of_8\": {\n",
    "        \"sampler\": StandardSampler(temperature=0.7, top_p=0.95, max_tokens=2048),\n",
    "        \"evaluator\": BestOfNEvaluator(dataset),\n",
    "        \"n_samples\": 8,\n",
    "        \"description\": \"Best-of-8 by log probability\",\n",
    "    },\n",
    "    \n",
    "    # Weighted voting by probability\n",
    "    \"weighted_voting_8\": {\n",
    "        \"sampler\": StandardSampler(temperature=0.7, top_p=0.95, max_tokens=2048),\n",
    "        \"evaluator\": WeightedVotingEvaluator(dataset),\n",
    "        \"n_samples\": 8,\n",
    "        \"description\": \"Weighted voting by probability (8 samples)\",\n",
    "    },\n",
    "    \n",
    "    # Best-first tree search\n",
    "    \"best_first_tree\": {\n",
    "        \"sampler\": BestFirstTreeSearch(\n",
    "            max_expansions=30,\n",
    "            branch_factor=3,\n",
    "            max_tokens=512,\n",
    "            tokens_per_step=64,\n",
    "            temperature=0.7,\n",
    "        ),\n",
    "        \"evaluator\": AccuracyEvaluator(dataset),\n",
    "        \"n_samples\": 1,\n",
    "        \"description\": \"Best-first tree search\",\n",
    "    },\n",
    "    \n",
    "    # MCTS tree search\n",
    "    \"mcts_tree\": {\n",
    "        \"sampler\": MCTSTreeSearch(\n",
    "            max_iterations=50,\n",
    "            branch_factor=3,\n",
    "            max_tokens=512,\n",
    "            tokens_per_step=64,\n",
    "            rollout_tokens=128,\n",
    "            temperature=0.7,\n",
    "        ),\n",
    "        \"evaluator\": AccuracyEvaluator(dataset),\n",
    "        \"n_samples\": 1,\n",
    "        \"description\": \"MCTS tree search\",\n",
    "    },\n",
    "}\n",
    "\n",
    "print(f\"Defined {len(experiments)} experiments:\")\n",
    "for name, exp in experiments.items():\n",
    "    print(f\"  - {name}: {exp['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "all_results = {}\n",
    "\n",
    "for exp_name, exp_config in experiments.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running: {exp_name}\")\n",
    "    print(f\"Description: {exp_config['description']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    results, metrics, responses, scores = run_evaluation(\n",
    "        model=model,\n",
    "        sampler=exp_config[\"sampler\"],\n",
    "        dataset=dataset,\n",
    "        evaluator=exp_config[\"evaluator\"],\n",
    "        batch_size=4 if exp_config[\"n_samples\"] > 1 else 8,\n",
    "        n_samples=exp_config[\"n_samples\"],\n",
    "        max_problems=MAX_PROBLEMS,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    elapsed = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    # Save results\n",
    "    run_dir = save_results(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        run_name=exp_name,\n",
    "        results=results,\n",
    "        metrics=metrics,\n",
    "        config={\n",
    "            \"experiment\": exp_name,\n",
    "            \"description\": exp_config[\"description\"],\n",
    "            \"n_samples\": exp_config[\"n_samples\"],\n",
    "            \"model\": MODEL_NAME,\n",
    "        },\n",
    "        responses=responses,\n",
    "        scores=scores,\n",
    "    )\n",
    "    \n",
    "    all_results[exp_name] = {\n",
    "        \"metrics\": metrics,\n",
    "        \"results\": results,\n",
    "        \"elapsed\": elapsed,\n",
    "        \"run_dir\": run_dir,\n",
    "        \"n_samples\": exp_config[\"n_samples\"],\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nCompleted in {elapsed:.1f}s\")\n",
    "    print(f\"Accuracy: {metrics.accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "import pandas as pd\n",
    "\n",
    "comparison_data = []\n",
    "for exp_name, data in all_results.items():\n",
    "    metrics = data[\"metrics\"]\n",
    "    comparison_data.append({\n",
    "        \"Experiment\": exp_name,\n",
    "        \"Description\": experiments[exp_name][\"description\"],\n",
    "        \"Accuracy\": metrics.accuracy,\n",
    "        \"Correct\": metrics.correct,\n",
    "        \"Total\": metrics.total,\n",
    "        \"N Samples\": data[\"n_samples\"],\n",
    "        \"Time (s)\": data[\"elapsed\"],\n",
    "        \"Time/Problem (s)\": data[\"elapsed\"] / metrics.total,\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "df = df.sort_values(\"Accuracy\", ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy comparison bar chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy\n",
    "ax1 = axes[0]\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(df)))\n",
    "bars = ax1.barh(df[\"Experiment\"], df[\"Accuracy\"], color=colors)\n",
    "ax1.set_xlabel(\"Accuracy\")\n",
    "ax1.set_title(\"GSM8K Accuracy by Strategy\")\n",
    "ax1.set_xlim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, df[\"Accuracy\"]):\n",
    "    ax1.text(acc + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "             f'{acc:.3f}', va='center', fontsize=10)\n",
    "\n",
    "# Compute efficiency (accuracy per second)\n",
    "ax2 = axes[1]\n",
    "df[\"Efficiency\"] = df[\"Accuracy\"] / df[\"Time/Problem (s)\"]\n",
    "bars = ax2.barh(df[\"Experiment\"], df[\"Efficiency\"], color=colors)\n",
    "ax2.set_xlabel(\"Accuracy / Time per Problem\")\n",
    "ax2.set_title(\"Compute Efficiency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/accuracy_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy vs Compute tradeoff\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for exp_name, data in all_results.items():\n",
    "    metrics = data[\"metrics\"]\n",
    "    ax.scatter(\n",
    "        data[\"elapsed\"] / metrics.total,  # Time per problem\n",
    "        metrics.accuracy,\n",
    "        s=100 * data[\"n_samples\"],  # Size by n_samples\n",
    "        label=exp_name,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    ax.annotate(\n",
    "        exp_name,\n",
    "        (data[\"elapsed\"] / metrics.total, metrics.accuracy),\n",
    "        xytext=(5, 5),\n",
    "        textcoords='offset points',\n",
    "        fontsize=9,\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"Time per Problem (seconds)\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Accuracy vs Compute Tradeoff\\n(bubble size = number of samples)\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/accuracy_vs_compute.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Self-Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze agreement rates for self-consistency methods\n",
    "sc_experiments = [\"self_consistency_8\", \"self_consistency_16\", \"diverse_consistency_8\"]\n",
    "\n",
    "for exp_name in sc_experiments:\n",
    "    if exp_name not in all_results:\n",
    "        continue\n",
    "        \n",
    "    results = all_results[exp_name][\"results\"]\n",
    "    \n",
    "    # Extract agreement info\n",
    "    agreements = []\n",
    "    for r in results:\n",
    "        if r.metadata and \"vote_counts\" in r.metadata:\n",
    "            vote_counts = r.metadata[\"vote_counts\"]\n",
    "            if vote_counts:\n",
    "                max_votes = max(vote_counts.values())\n",
    "                total = r.metadata.get(\"num_samples\", sum(vote_counts.values()))\n",
    "                agreements.append(max_votes / total)\n",
    "    \n",
    "    if agreements:\n",
    "        print(f\"\\n{exp_name}:\")\n",
    "        print(f\"  Mean agreement rate: {np.mean(agreements):.3f}\")\n",
    "        print(f\"  Std agreement rate: {np.std(agreements):.3f}\")\n",
    "        print(f\"  Min/Max: {np.min(agreements):.3f} / {np.max(agreements):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agreement vs correctness analysis\n",
    "if \"self_consistency_16\" in all_results:\n",
    "    results = all_results[\"self_consistency_16\"][\"results\"]\n",
    "    \n",
    "    correct_agreements = []\n",
    "    incorrect_agreements = []\n",
    "    \n",
    "    for r in results:\n",
    "        if r.metadata and \"vote_counts\" in r.metadata:\n",
    "            vote_counts = r.metadata[\"vote_counts\"]\n",
    "            if vote_counts:\n",
    "                max_votes = max(vote_counts.values())\n",
    "                total = r.metadata.get(\"num_samples\", 16)\n",
    "                agreement = max_votes / total\n",
    "                \n",
    "                if r.correct:\n",
    "                    correct_agreements.append(agreement)\n",
    "                else:\n",
    "                    incorrect_agreements.append(agreement)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    ax.hist(correct_agreements, bins=20, alpha=0.7, label=f'Correct (n={len(correct_agreements)})', color='green')\n",
    "    ax.hist(incorrect_agreements, bins=20, alpha=0.7, label=f'Incorrect (n={len(incorrect_agreements)})', color='red')\n",
    "    \n",
    "    ax.set_xlabel(\"Agreement Rate (max votes / total samples)\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(\"Self-Consistency: Agreement Rate Distribution by Correctness\")\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/agreement_vs_correctness.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nMean agreement when correct: {np.mean(correct_agreements):.3f}\")\n",
    "    print(f\"Mean agreement when incorrect: {np.mean(incorrect_agreements):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find problems where strategies disagree\n",
    "greedy_results = {r.problem_id: r.correct for r in all_results[\"greedy\"][\"results\"]}\n",
    "sc_results = {r.problem_id: r.correct for r in all_results.get(\"self_consistency_8\", {}).get(\"results\", [])}\n",
    "\n",
    "if sc_results:\n",
    "    # Problems where self-consistency helped\n",
    "    sc_helped = [pid for pid in greedy_results if not greedy_results[pid] and sc_results.get(pid, False)]\n",
    "    # Problems where self-consistency hurt\n",
    "    sc_hurt = [pid for pid in greedy_results if greedy_results[pid] and not sc_results.get(pid, True)]\n",
    "    \n",
    "    print(f\"Self-consistency helped on {len(sc_helped)} problems\")\n",
    "    print(f\"Self-consistency hurt on {len(sc_hurt)} problems\")\n",
    "    print(f\"Net improvement: {len(sc_helped) - len(sc_hurt)} problems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze difficult problems (wrong across all methods)\n",
    "all_wrong = set()\n",
    "for exp_name, data in all_results.items():\n",
    "    wrong_ids = {r.problem_id for r in data[\"results\"] if not r.correct}\n",
    "    if not all_wrong:\n",
    "        all_wrong = wrong_ids\n",
    "    else:\n",
    "        all_wrong = all_wrong.intersection(wrong_ids)\n",
    "\n",
    "print(f\"\\nProblems wrong across ALL strategies: {len(all_wrong)}\")\n",
    "\n",
    "# Show a few examples\n",
    "if all_wrong:\n",
    "    print(\"\\nExample hard problems:\")\n",
    "    for pid in list(all_wrong)[:3]:\n",
    "        for p in problems:\n",
    "            if p.id == pid:\n",
    "                print(f\"\\n--- {pid} ---\")\n",
    "                print(f\"Question: {p.prompt[:300]}...\")\n",
    "                print(f\"Gold answer: {p.gold_answer}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary & Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: GSM8K Strategy Comparison\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_df = df[[\"Experiment\", \"Accuracy\", \"N Samples\", \"Time/Problem (s)\"]].copy()\n",
    "summary_df[\"Accuracy\"] = summary_df[\"Accuracy\"].apply(lambda x: f\"{x:.4f}\")\n",
    "summary_df[\"Time/Problem (s)\"] = summary_df[\"Time/Problem (s)\"].apply(lambda x: f\"{x:.2f}\")\n",
    "\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Key findings\n",
    "best_exp = df.iloc[0]\n",
    "print(f\"\\n\\nKey Findings:\")\n",
    "print(f\"  1. Best accuracy: {best_exp['Experiment']} ({best_exp['Accuracy']:.4f})\")\n",
    "print(f\"  2. Greedy baseline: {all_results['greedy']['metrics'].accuracy:.4f}\")\n",
    "\n",
    "if \"self_consistency_8\" in all_results:\n",
    "    sc_acc = all_results[\"self_consistency_8\"][\"metrics\"].accuracy\n",
    "    greedy_acc = all_results[\"greedy\"][\"metrics\"].accuracy\n",
    "    print(f\"  3. Self-consistency improvement: +{sc_acc - greedy_acc:.4f} ({(sc_acc - greedy_acc) / greedy_acc * 100:.1f}%)\")\n",
    "\n",
    "# Save summary\n",
    "df.to_csv(f\"{OUTPUT_DIR}/summary.csv\", index=False)\n",
    "print(f\"\\nResults saved to {OUTPUT_DIR}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
