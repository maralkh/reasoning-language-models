{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HumanEval: Code Generation Strategies\n",
    "\n",
    "This notebook compares different sampling strategies on HumanEval, focusing on **pass@k** metrics.\n",
    "\n",
    "**Key aspects:**\n",
    "- Code generation is different from math - correctness is binary (passes tests or not)\n",
    "- pass@k is the standard metric: probability of at least one correct solution in k samples\n",
    "- Temperature and diversity matter for exploration\n",
    "\n",
    "**Strategies compared:**\n",
    "- Greedy (pass@1 baseline)\n",
    "- Temperature sampling (pass@1, 5, 10)\n",
    "- Various temperatures and top_p values\n",
    "- Nucleus sampling configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "from src.models import VLLMModel\n",
    "from src.datasets import HumanEvalDataset\n",
    "from src.samplers import GreedySampler, StandardSampler, NucleusSampler, DiverseSampler\n",
    "from src.evaluators import PassAtKEvaluator, AccuracyEvaluator\n",
    "from src.runners import run_evaluation\n",
    "from src.utils import save_results\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "MAX_PROBLEMS = 50  # HumanEval has 164 problems\n",
    "OUTPUT_DIR = \"../results/humaneval_comparison\"\n",
    "\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading model...\")\n",
    "model = VLLMModel(\n",
    "    model_name=MODEL_NAME,\n",
    "    gpu_memory_utilization=0.9,\n",
    ")\n",
    "print(f\"Model loaded: {model.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = HumanEvalDataset(timeout=5.0)\n",
    "print(f\"Dataset: {dataset.name}, {len(dataset)} problems\")\n",
    "\n",
    "problems = dataset.get_problems(limit=MAX_PROBLEMS)\n",
    "print(f\"Using {len(problems)} problems\")\n",
    "\n",
    "# Preview\n",
    "print(f\"\\nExample problem:\")\n",
    "print(problems[0].prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Experiments\n",
    "\n",
    "For code generation, we focus on pass@k with different sampling configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll run with n=20 samples for all temperature experiments to compute pass@1,5,10\n",
    "N_SAMPLES = 20\n",
    "\n",
    "experiments = {\n",
    "    # Baseline greedy\n",
    "    \"greedy\": {\n",
    "        \"sampler\": GreedySampler(max_tokens=512),\n",
    "        \"evaluator\": AccuracyEvaluator(dataset),\n",
    "        \"n_samples\": 1,\n",
    "        \"description\": \"Greedy (pass@1 only)\",\n",
    "    },\n",
    "    \n",
    "    # Temperature 0.2 (low diversity)\n",
    "    \"temp_0.2\": {\n",
    "        \"sampler\": StandardSampler(temperature=0.2, top_p=0.95, max_tokens=512),\n",
    "        \"evaluator\": PassAtKEvaluator(dataset, k_values=[1, 5, 10]),\n",
    "        \"n_samples\": N_SAMPLES,\n",
    "        \"description\": \"Temperature 0.2\",\n",
    "    },\n",
    "    \n",
    "    # Temperature 0.4\n",
    "    \"temp_0.4\": {\n",
    "        \"sampler\": StandardSampler(temperature=0.4, top_p=0.95, max_tokens=512),\n",
    "        \"evaluator\": PassAtKEvaluator(dataset, k_values=[1, 5, 10]),\n",
    "        \"n_samples\": N_SAMPLES,\n",
    "        \"description\": \"Temperature 0.4\",\n",
    "    },\n",
    "    \n",
    "    # Temperature 0.6\n",
    "    \"temp_0.6\": {\n",
    "        \"sampler\": StandardSampler(temperature=0.6, top_p=0.95, max_tokens=512),\n",
    "        \"evaluator\": PassAtKEvaluator(dataset, k_values=[1, 5, 10]),\n",
    "        \"n_samples\": N_SAMPLES,\n",
    "        \"description\": \"Temperature 0.6\",\n",
    "    },\n",
    "    \n",
    "    # Temperature 0.8 (recommended by Codex paper)\n",
    "    \"temp_0.8\": {\n",
    "        \"sampler\": StandardSampler(temperature=0.8, top_p=0.95, max_tokens=512),\n",
    "        \"evaluator\": PassAtKEvaluator(dataset, k_values=[1, 5, 10]),\n",
    "        \"n_samples\": N_SAMPLES,\n",
    "        \"description\": \"Temperature 0.8 (Codex default)\",\n",
    "    },\n",
    "    \n",
    "    # Temperature 1.0\n",
    "    \"temp_1.0\": {\n",
    "        \"sampler\": StandardSampler(temperature=1.0, top_p=0.95, max_tokens=512),\n",
    "        \"evaluator\": PassAtKEvaluator(dataset, k_values=[1, 5, 10]),\n",
    "        \"n_samples\": N_SAMPLES,\n",
    "        \"description\": \"Temperature 1.0\",\n",
    "    },\n",
    "    \n",
    "    # Nucleus sampling p=0.9\n",
    "    \"nucleus_0.9\": {\n",
    "        \"sampler\": NucleusSampler(temperature=0.8, top_p=0.9, max_tokens=512),\n",
    "        \"evaluator\": PassAtKEvaluator(dataset, k_values=[1, 5, 10]),\n",
    "        \"n_samples\": N_SAMPLES,\n",
    "        \"description\": \"Nucleus p=0.9, temp=0.8\",\n",
    "    },\n",
    "    \n",
    "    # Nucleus sampling p=0.95\n",
    "    \"nucleus_0.95\": {\n",
    "        \"sampler\": NucleusSampler(temperature=0.8, top_p=0.95, max_tokens=512),\n",
    "        \"evaluator\": PassAtKEvaluator(dataset, k_values=[1, 5, 10]),\n",
    "        \"n_samples\": N_SAMPLES,\n",
    "        \"description\": \"Nucleus p=0.95, temp=0.8\",\n",
    "    },\n",
    "    \n",
    "    # Diverse temperatures\n",
    "    \"diverse\": {\n",
    "        \"sampler\": DiverseSampler(\n",
    "            temperatures=[0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "            top_p=0.95,\n",
    "            max_tokens=512\n",
    "        ),\n",
    "        \"evaluator\": PassAtKEvaluator(dataset, k_values=[1, 5, 10]),\n",
    "        \"n_samples\": N_SAMPLES,\n",
    "        \"description\": \"Diverse temperatures\",\n",
    "    },\n",
    "}\n",
    "\n",
    "print(f\"Defined {len(experiments)} experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "\n",
    "for exp_name, exp_config in experiments.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running: {exp_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    results, metrics, responses, scores = run_evaluation(\n",
    "        model=model,\n",
    "        sampler=exp_config[\"sampler\"],\n",
    "        dataset=dataset,\n",
    "        evaluator=exp_config[\"evaluator\"],\n",
    "        batch_size=2 if exp_config[\"n_samples\"] > 1 else 8,\n",
    "        n_samples=exp_config[\"n_samples\"],\n",
    "        max_problems=MAX_PROBLEMS,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    elapsed = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    run_dir = save_results(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        run_name=exp_name,\n",
    "        results=results,\n",
    "        metrics=metrics,\n",
    "        config={\"experiment\": exp_name, \"n_samples\": exp_config[\"n_samples\"]},\n",
    "        responses=responses,\n",
    "        scores=scores,\n",
    "    )\n",
    "    \n",
    "    all_results[exp_name] = {\n",
    "        \"metrics\": metrics,\n",
    "        \"results\": results,\n",
    "        \"elapsed\": elapsed,\n",
    "        \"n_samples\": exp_config[\"n_samples\"],\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nCompleted in {elapsed:.1f}s\")\n",
    "    print(f\"Metrics: {metrics.metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare pass@k Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract pass@k metrics\n",
    "comparison_data = []\n",
    "for exp_name, data in all_results.items():\n",
    "    metrics = data[\"metrics\"]\n",
    "    row = {\n",
    "        \"Experiment\": exp_name,\n",
    "        \"Description\": experiments[exp_name][\"description\"],\n",
    "        \"N Samples\": data[\"n_samples\"],\n",
    "        \"Time (s)\": data[\"elapsed\"],\n",
    "    }\n",
    "    \n",
    "    # Add pass@k metrics\n",
    "    if \"pass@1\" in metrics.metrics:\n",
    "        row[\"pass@1\"] = metrics.metrics[\"pass@1\"]\n",
    "        row[\"pass@5\"] = metrics.metrics.get(\"pass@5\", np.nan)\n",
    "        row[\"pass@10\"] = metrics.metrics.get(\"pass@10\", np.nan)\n",
    "    else:\n",
    "        # Greedy is just accuracy\n",
    "        row[\"pass@1\"] = metrics.accuracy\n",
    "        row[\"pass@5\"] = np.nan\n",
    "        row[\"pass@10\"] = np.nan\n",
    "    \n",
    "    comparison_data.append(row)\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "df = df.sort_values(\"pass@1\", ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass@k comparison chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Filter to experiments with multiple samples\n",
    "df_multi = df[df[\"N Samples\"] > 1].copy()\n",
    "\n",
    "x = np.arange(len(df_multi))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax.bar(x - width, df_multi[\"pass@1\"], width, label='pass@1', color='#3498db')\n",
    "bars2 = ax.bar(x, df_multi[\"pass@5\"], width, label='pass@5', color='#2ecc71')\n",
    "bars3 = ax.bar(x + width, df_multi[\"pass@10\"], width, label='pass@10', color='#e74c3c')\n",
    "\n",
    "ax.set_xlabel('Sampling Strategy')\n",
    "ax.set_ylabel('pass@k')\n",
    "ax.set_title('HumanEval: pass@k by Sampling Strategy')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df_multi[\"Experiment\"], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        if not np.isnan(height):\n",
    "            ax.annotate(f'{height:.2f}',\n",
    "                       xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                       xytext=(0, 3),\n",
    "                       textcoords=\"offset points\",\n",
    "                       ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/pass_at_k_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature vs pass@k curve\n",
    "temp_experiments = [\"temp_0.2\", \"temp_0.4\", \"temp_0.6\", \"temp_0.8\", \"temp_1.0\"]\n",
    "temps = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "pass_at_1 = [all_results[exp][\"metrics\"].metrics.get(\"pass@1\", 0) for exp in temp_experiments if exp in all_results]\n",
    "pass_at_5 = [all_results[exp][\"metrics\"].metrics.get(\"pass@5\", 0) for exp in temp_experiments if exp in all_results]\n",
    "pass_at_10 = [all_results[exp][\"metrics\"].metrics.get(\"pass@10\", 0) for exp in temp_experiments if exp in all_results]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(temps[:len(pass_at_1)], pass_at_1, 'o-', label='pass@1', markersize=10, linewidth=2)\n",
    "ax.plot(temps[:len(pass_at_5)], pass_at_5, 's-', label='pass@5', markersize=10, linewidth=2)\n",
    "ax.plot(temps[:len(pass_at_10)], pass_at_10, '^-', label='pass@10', markersize=10, linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Temperature', fontsize=12)\n",
    "ax.set_ylabel('pass@k', fontsize=12)\n",
    "ax.set_title('Temperature vs pass@k', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/temperature_curve.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Find optimal temperature\n",
    "if pass_at_1:\n",
    "    best_temp_idx = np.argmax(pass_at_1)\n",
    "    print(f\"\\nOptimal temperature for pass@1: {temps[best_temp_idx]} (pass@1 = {pass_at_1[best_temp_idx]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Sample Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze how many unique solutions are generated per problem\n",
    "def count_unique_solutions(results):\n",
    "    \"\"\"Count unique correct solutions per problem.\"\"\"\n",
    "    unique_counts = []\n",
    "    for r in results:\n",
    "        if r.metadata and \"correct_flags\" in r.metadata:\n",
    "            n_correct = sum(r.metadata[\"correct_flags\"])\n",
    "            unique_counts.append(n_correct)\n",
    "    return unique_counts\n",
    "\n",
    "# Compare diversity across temperatures\n",
    "for exp_name in [\"temp_0.2\", \"temp_0.6\", \"temp_1.0\"]:\n",
    "    if exp_name not in all_results:\n",
    "        continue\n",
    "    results = all_results[exp_name][\"results\"]\n",
    "    unique_counts = count_unique_solutions(results)\n",
    "    \n",
    "    if unique_counts:\n",
    "        print(f\"\\n{exp_name}:\")\n",
    "        print(f\"  Mean correct samples per problem: {np.mean(unique_counts):.2f}\")\n",
    "        print(f\"  Max correct samples: {max(unique_counts)}\")\n",
    "        print(f\"  Problems with 0 correct: {sum(1 for c in unique_counts if c == 0)}\")\n",
    "        print(f\"  Problems with 1+ correct: {sum(1 for c in unique_counts if c >= 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of correct samples per problem\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, exp_name in zip(axes, [\"temp_0.2\", \"temp_0.6\", \"temp_1.0\"]):\n",
    "    if exp_name not in all_results:\n",
    "        continue\n",
    "        \n",
    "    results = all_results[exp_name][\"results\"]\n",
    "    unique_counts = count_unique_solutions(results)\n",
    "    \n",
    "    if unique_counts:\n",
    "        ax.hist(unique_counts, bins=range(N_SAMPLES + 2), align='left', rwidth=0.8)\n",
    "        ax.set_xlabel('Number of Correct Samples')\n",
    "        ax.set_ylabel('Number of Problems')\n",
    "        ax.set_title(f'{exp_name}\\nMean: {np.mean(unique_counts):.2f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/sample_diversity.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: HumanEval Code Generation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Summary table\n",
    "summary_df = df[[\"Experiment\", \"pass@1\", \"pass@5\", \"pass@10\", \"N Samples\"]].copy()\n",
    "for col in [\"pass@1\", \"pass@5\", \"pass@10\"]:\n",
    "    summary_df[col] = summary_df[col].apply(lambda x: f\"{x:.4f}\" if not np.isnan(x) else \"-\")\n",
    "\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Key findings\n",
    "print(f\"\\n\\nKey Findings:\")\n",
    "print(f\"  1. Greedy pass@1: {all_results.get('greedy', {}).get('metrics', {}).accuracy if 'greedy' in all_results else 'N/A'}\")\n",
    "\n",
    "# Best temperature for each k\n",
    "df_multi = df[df[\"N Samples\"] > 1]\n",
    "if not df_multi.empty:\n",
    "    best_pass1 = df_multi.loc[df_multi[\"pass@1\"].idxmax()]\n",
    "    print(f\"  2. Best pass@1: {best_pass1['Experiment']} ({best_pass1['pass@1']:.4f})\")\n",
    "    \n",
    "    if \"pass@10\" in df_multi.columns and not df_multi[\"pass@10\"].isna().all():\n",
    "        best_pass10 = df_multi.loc[df_multi[\"pass@10\"].idxmax()]\n",
    "        print(f\"  3. Best pass@10: {best_pass10['Experiment']} ({best_pass10['pass@10']:.4f})\")\n",
    "\n",
    "df.to_csv(f\"{OUTPUT_DIR}/summary.csv\", index=False)\n",
    "print(f\"\\nResults saved to {OUTPUT_DIR}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
